import os, datetime, numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
from battle_ssafy_env import BattleSsafyEnv
from tqdm import tqdm

# í™˜ê²½ ìƒìˆ˜ë“¤ (BattleSsafyEnvì—ì„œ ê°€ì ¸ì˜´)
G, R, W, F, X, E, S, T, H = 0, 1, 2, 3, 4, 5, 6, 7, 8

class TqdmCallback(BaseCallback):
    def __init__(self, verbose: int = 0):
        super().__init__(verbose)
        self.pbar = None
        self.start_num = 0
        self.goal = 0

    def _on_training_start(self) -> None:
        self.start_num = int(self.model.num_timesteps)
        to_add = int(getattr(self.model, "_total_timesteps", 0))
        self.goal = self.start_num + to_add

        self.pbar = tqdm(
            total=self.goal,
            initial=self.start_num,
            desc="Training",
            unit="steps",
            dynamic_ncols=True,
            mininterval=0.2,
            leave=True,
        )

    def _on_step(self) -> bool:
        if self.pbar:
            curr = int(self.model.num_timesteps)
            if curr > self.pbar.n:
                self.pbar.update(curr - self.pbar.n)
        return True

    def _on_training_end(self) -> None:
        if self.pbar:
            curr = int(self.model.num_timesteps)
            if curr > self.pbar.n:
                self.pbar.update(curr - self.pbar.n)
            self.pbar.close()


class EfficientEvalCallback(EvalCallback):
    """íš¨ìœ¨ì ì¸ í‰ê°€ ì½œë°± - ì—ì´ì „íŠ¸ ê²½ë¡œë§Œ ì‹œê°í™”"""
    
    def __init__(self, eval_env, plot_dir="./eval_plots", **kwargs):
        super().__init__(eval_env=eval_env, **kwargs)
        self.plot_dir = plot_dir
        os.makedirs(self.plot_dir, exist_ok=True)
        
        # ì‹œê°í™” ì„¤ì •
        sns.set_theme()
        plt.style.use('seaborn-v0_8')

    def _on_step(self) -> bool:
        continue_training = super()._on_step()
        
        # í‰ê°€ê°€ ì‹¤ì œë¡œ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            self._create_path_visualization()
        
        return continue_training

    def _create_path_visualization(self):
        """ì—ì´ì „íŠ¸ ê²½ë¡œë¥¼ ì‹œê°í™”í•œ ì •ì  ì´ë¯¸ì§€ ìƒì„±"""
        try:
            # ìƒˆ í™˜ê²½ì—ì„œ ì—í”¼ì†Œë“œ ì‹¤í–‰í•˜ì—¬ ê²½ë¡œ ìˆ˜ì§‘
            test_env = BattleSsafyEnv(size=16, render_mode=None)
            obs, _ = test_env.reset()
            
            # ê²½ë¡œ ì¶”ì 
            agent_path = []
            done, truncated = False, False
            max_steps = 200
            step_count = 0
            
            while not (done or truncated) and step_count < max_steps:
                # í˜„ì¬ ìœ„ì¹˜ ì €ì¥
                agent_path.append(tuple(test_env._agent_pos))
                
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = test_env.step(action)
                step_count += 1
            
            # ë§ˆì§€ë§‰ ìœ„ì¹˜ ì¶”ê°€
            if step_count < max_steps:
                agent_path.append(tuple(test_env._agent_pos))
            
            # ì‹œê°í™” ìƒì„±
            self._plot_agent_path(test_env, agent_path, step_count, done)
            test_env.close()
            
        except Exception as e:
            print(f"[EfficientEvalCallback] ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    def _plot_agent_path(self, env, agent_path, steps, success):
        """ì—ì´ì „íŠ¸ ê²½ë¡œë¥¼ ë§µ ìœ„ì— ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. ë§µ + ê²½ë¡œ ì‹œê°í™”
        self._plot_map_with_path(ax1, env, agent_path, success)
        
        # 2. ë°©ë¬¸ ë¹ˆë„ íˆíŠ¸ë§µ
        self._plot_visit_heatmap(ax2, agent_path, env.H, env.W)
        
        # ì œëª©ê³¼ ì •ë³´
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        reward_info = f"Steps: {steps}, Success: {success}"
        fig.suptitle(f'Agent Evaluation - {self.num_timesteps} timesteps\n{reward_info}', 
                    fontsize=14, fontweight='bold')
        
        # ì €ì¥
        plot_path = os.path.join(self.plot_dir, f"eval_{self.num_timesteps}_{ts}.png")
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"[EfficientEvalCallback] í‰ê°€ ê²°ê³¼ ì €ì¥: {plot_path}")

    def _plot_map_with_path(self, ax, env, agent_path, success):
        """ë§µê³¼ ì—ì´ì „íŠ¸ ê²½ë¡œ ì‹œê°í™”"""
        # íƒ€ì¼ ìƒ‰ìƒ ë§µ
        tile_colors = {
            G: 'lightgray',   # í‰ì§€
            R: 'darkgray',    # ë°”ìœ„
            W: 'lightblue',   # ë¬¼
            F: 'gold',        # ë³´ê¸‰ì†Œ
            X: 'red',         # ì  í¬íƒ‘
            E: 'blue',        # ì 
            T: 'green',       # ë‚˜ë¬´
            S: 'wheat',       # ëª¨ë˜
            H: 'lightgreen',  # ìš°ë¦¬ í¬íƒ‘
        }
        
        # ë² ì´ìŠ¤ ë§µ ì‹œê°í™”
        colored_map = np.zeros((env.H, env.W, 3))
        for y in range(env.H):
            for x in range(env.W):
                tile_type = env._base_map[y, x]
                color_name = tile_colors.get(tile_type, 'white')
                # matplotlib ìƒ‰ìƒì„ RGBë¡œ ë³€í™˜
                try:
                    color_rgb = plt.colors.to_rgb(color_name)
                    colored_map[y, x] = color_rgb
                except:
                    colored_map[y, x] = [1, 1, 1]  # ê¸°ë³¸ í°ìƒ‰
        
        # ë§µ í‘œì‹œ (yì¶• ë’¤ì§‘ê¸° - ì›ì ì´ ì™¼ìª½ ìœ„)
        ax.imshow(colored_map, origin='upper')
        
        # í¬íƒ‘ ìœ„ì¹˜ í‘œì‹œ
        for tx, ty in env._turrets:
            ax.plot(tx, ty, 'rX', markersize=12, markeredgewidth=3)
        
        for hx, hy in env._home_turrets:
            ax.plot(hx, hy, 'gX', markersize=12, markeredgewidth=3)
        
        # ì—ì´ì „íŠ¸ ê²½ë¡œ í‘œì‹œ
        if len(agent_path) > 1:
            path_x = [pos[0] for pos in agent_path]
            path_y = [pos[1] for pos in agent_path]
            
            # ê²½ë¡œ ì„ 
            ax.plot(path_x, path_y, 'orange', linewidth=3, alpha=0.8, label='Agent Path')
            
            # ì‹œì‘ì 
            ax.plot(path_x[0], path_y[0], 'go', markersize=10, label='Start')
            
            # ëì 
            end_color = 'lime' if success else 'orange'
            ax.plot(path_x[-1], path_y[-1], 'o', color=end_color, markersize=10, label='End')
            
            # ë°©í–¥ í™”ì‚´í‘œ (ëª‡ ê°œë§Œ í‘œì‹œ)
            step_interval = max(1, len(agent_path) // 8)
            for i in range(0, len(agent_path)-1, step_interval):
                dx = path_x[i+1] - path_x[i]
                dy = path_y[i+1] - path_y[i]
                if dx != 0 or dy != 0:
                    ax.arrow(path_x[i], path_y[i], dx*0.3, dy*0.3, 
                            head_width=0.2, head_length=0.2, 
                            fc='darkorange', ec='darkorange', alpha=0.7)
        
        ax.set_xlim(-0.5, env.W-0.5)
        ax.set_ylim(-0.5, env.H-0.5)
        ax.set_title('Map with Agent Path')
        ax.legend(loc='upper right', bbox_to_anchor=(1.15, 1))
        ax.grid(True, alpha=0.3)
        ax.set_aspect('equal')

    def _plot_visit_heatmap(self, ax, agent_path, height, width):
        """ë°©ë¬¸ ë¹ˆë„ íˆíŠ¸ë§µ"""
        visit_count = np.zeros((height, width))
        
        # ë°©ë¬¸ íšŸìˆ˜ ê³„ì‚°
        for x, y in agent_path:
            if 0 <= x < width and 0 <= y < height:
                visit_count[y, x] += 1
        
        # íˆíŠ¸ë§µ ìƒì„±
        sns.heatmap(visit_count, annot=True, fmt='d', cmap='YlOrRd', 
                   ax=ax, cbar_kws={'label': 'Visit Count'})
        ax.set_title('Visit Frequency Heatmap')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')


class PolicyVisualizationCallback(BaseCallback):
    """ì •ì±… ì‹œê°í™” ì½œë°± - í•™ìŠµëœ ì •ì±…ì„ í™”ì‚´í‘œë¡œ í‘œì‹œ"""
    
    def __init__(self, plot_dir="./policy_plots", save_freq=10000, verbose=1):
        super().__init__(verbose)
        self.plot_dir = plot_dir
        self.save_freq = save_freq
        os.makedirs(self.plot_dir, exist_ok=True)

    def _on_step(self) -> bool:
        if self.n_calls % self.save_freq == 0:
            self._visualize_policy()
        return True

    def _visualize_policy(self):
        """í•™ìŠµëœ ì •ì±…ì„ ì‹œê°í™”"""
        try:
            # í™˜ê²½ì˜ ê° ìƒíƒœì—ì„œ ìµœì  ì•¡ì…˜ ê³„ì‚°
            test_env = BattleSsafyEnv(size=16, render_mode=None)
            
            # ê° ìœ„ì¹˜ì—ì„œì˜ ìµœì  ì•¡ì…˜ ê³„ì‚°
            action_map = np.zeros((test_env.H, test_env.W), dtype=int)
            value_map = np.zeros((test_env.H, test_env.W))
            
            for y in range(test_env.H):
                for x in range(test_env.W):
                    if test_env._is_passable((x, y)):
                        # í•´ë‹¹ ìœ„ì¹˜ì—ì„œì˜ ê´€ì¸¡ê°’ ìƒì„±
                        test_env._agent_pos = np.array([x, y])
                        obs = test_env._get_obs()
                        
                        # ì •ì±…ìœ¼ë¡œë¶€í„° ì•¡ì…˜ í™•ë¥  ê³„ì‚°
                        action_probs = self.model.policy.get_distribution(obs).distribution.probs
                        best_action = int(np.argmax(action_probs.detach().cpu().numpy()))
                        max_prob = float(np.max(action_probs.detach().cpu().numpy()))
                        
                        action_map[y, x] = best_action
                        value_map[y, x] = max_prob
            
            # ì‹œê°í™”
            self._plot_policy_arrows(test_env, action_map, value_map)
            test_env.close()
            
        except Exception as e:
            if self.verbose:
                print(f"[PolicyVisualizationCallback] ì˜¤ë¥˜: {e}")

    def _plot_policy_arrows(self, env, action_map, value_map):
        """ì •ì±…ì„ í™”ì‚´í‘œë¡œ ì‹œê°í™”"""
        fig, ax = plt.subplots(figsize=(12, 12))
        
        # ì•¡ì…˜ì„ í™”ì‚´í‘œë¡œ ë§¤í•‘
        action_arrows = {0: 'â†’', 1: 'â†‘', 2: 'â†', 3: 'â†“', 4: 'ğŸ’£', 5: 'ğŸ’¥', 6: 'ğŸ”“'}
        
        # ë°°ê²½ ë§µ ê·¸ë¦¬ê¸° (ê°„ë‹¨í•œ ë²„ì „)
        for y in range(env.H):
            for x in range(env.W):
                tile_type = env._base_map[y, x]
                
                # íƒ€ì¼ì— ë”°ë¥¸ ìƒ‰ìƒ
                if tile_type == R:
                    ax.add_patch(plt.Rectangle((x-0.4, y-0.4), 0.8, 0.8, 
                                             facecolor='gray', alpha=0.5))
                elif tile_type == W:
                    ax.add_patch(plt.Rectangle((x-0.4, y-0.4), 0.8, 0.8, 
                                             facecolor='lightblue', alpha=0.5))
                elif tile_type == F:
                    ax.add_patch(plt.Rectangle((x-0.4, y-0.4), 0.8, 0.8, 
                                             facecolor='gold', alpha=0.5))
                
                # í†µí–‰ ê°€ëŠ¥í•œ ê³³ì—ë§Œ í™”ì‚´í‘œ í‘œì‹œ
                if env._is_passable((x, y)):
                    action = action_map[y, x]
                    confidence = value_map[y, x]
                    
                    # í™”ì‚´í‘œ ìƒ‰ìƒì€ ì‹ ë¢°ë„ì— ë”°ë¼
                    alpha = min(1.0, confidence * 2)
                    arrow = action_arrows.get(action, '?')
                    
                    ax.text(x, y, arrow, ha='center', va='center', 
                           fontsize=12, alpha=alpha, weight='bold')
        
        # í¬íƒ‘ í‘œì‹œ
        for tx, ty in env._turrets:
            ax.plot(tx, ty, 'rX', markersize=15, markeredgewidth=3)
        
        ax.set_xlim(-0.5, env.W-0.5)
        ax.set_ylim(-0.5, env.H-0.5)
        ax.set_title(f'Learned Policy - {self.num_timesteps} timesteps')
        ax.grid(True, alpha=0.3)
        ax.set_aspect('equal')
        
        # ì €ì¥
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(self.plot_dir, f"policy_{self.num_timesteps}_{ts}.png")
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        if self.verbose:
            print(f"[PolicyVisualizationCallback] ì •ì±… ì‹œê°í™” ì €ì¥: {plot_path}")